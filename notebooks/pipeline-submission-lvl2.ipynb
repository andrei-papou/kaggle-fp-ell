{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if 'KAGGLE_URL_BASE' in os.environ:\n",
    "    print('Running on Kaggle so initializing the environment...')\n",
    "\n",
    "    os.environ['__KGLTBX_ENVIRONMENT'] = 'kaggle'\n",
    "    sys.path.append('/kaggle/input/kaggle-toolbox')\n",
    "    sys.path.append('/kaggle/input/lib-textstat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "\n",
    "import kaggle_toolbox.features.generation as features\n",
    "import kaggle_toolbox.nlp.features as text_features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from catboost import CatBoostRegressor\n",
    "from kaggle_toolbox.data import DatasetItem, Movable\n",
    "from kaggle_toolbox.device import CUDADevice\n",
    "from kaggle_toolbox.ensembling import EnsemblingStrategy, MeanEnsemblingStrategy\n",
    "from kaggle_toolbox.environment import Environment\n",
    "from kaggle_toolbox.layers import SqueezeDim\n",
    "from kaggle_toolbox.nlp.transformer import Backbone, Model as TransformerModel, StandardModel, \\\n",
    "    MeanPooler, AttentionHeadPooler, TakeNthSqueezer, ConcatSqueezer, get_tokenizer_for_backbone, \\\n",
    "    Tokenizer, TokenizerResult, TokenizerResultCollator, seed_everything\n",
    "from kaggle_toolbox.prediction import PredDict\n",
    "from kaggle_toolbox.predictor import StandardPredictor\n",
    "from kaggle_toolbox.progress import NotebookProgressBar\n",
    "from kaggle_toolbox.typing import DynamicDict\n",
    "from textstat import textstat\n",
    "from torch.utils.data import Dataset as TorchDataset, default_collate as default_collate_fn\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "from transformers.utils.generic import PaddingStrategy\n",
    "from transformers.utils.logging import set_verbosity_error as set_transformers_verbosity_error\n",
    "\n",
    "\n",
    "NotebookProgressBar.attach_to_pandas()\n",
    "set_transformers_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = t.TypeVar('_X', bound=Movable)\n",
    "\n",
    "class DatasetItemCollator(t.Generic[_X]):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            x_collate_fn: t.Callable[[t.List[_X]], _X],\n",
    "            id_collate_fn: t.Callable[[t.List[t.List[str]]], t.List[str]] = default_collate_fn):\n",
    "        self._x_collate_fn = x_collate_fn\n",
    "        self._id_collate_fn = id_collate_fn\n",
    "\n",
    "    def __call__(self, item_list: t.List[DatasetItem[_X]]) -> DatasetItem[_X]:\n",
    "        return DatasetItem(\n",
    "            id=self._id_collate_fn([item.id for item in item_list]),\n",
    "            x=self._x_collate_fn([item.x for item in item_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(TorchDataset[DatasetItem[TokenizerResult]]):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            tokenizer: Tokenizer,\n",
    "            max_len: int):\n",
    "        self._df = df.copy().reset_index(drop=True)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "\n",
    "    def _get_tokenizer_input(self, row: DynamicDict) -> str:\n",
    "        (\n",
    "            full_text,\n",
    "         ) = (\n",
    "            row.get_typed_or_raise('full_text', str),\n",
    "         )\n",
    "\n",
    "        return full_text\n",
    "\n",
    "    def sort_by_tokenizer_input_len(self):\n",
    "        self._df['_tok_input_len'] = self._df.progress_apply(\n",
    "            lambda row: self._get_tokenizer_input(DynamicDict(t.cast(t.Dict[str, t.Any], row))), axis=1)\n",
    "        self._df = self._df.sort_values('_tok_input_len')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> DatasetItem[TokenizerResult]:\n",
    "        row = self._df.iloc[idx]\n",
    "\n",
    "        tokenizer_input = self._get_tokenizer_input(DynamicDict(t.cast(t.Dict[str, t.Any], row)))\n",
    "        id = str(row['text_id'])\n",
    "\n",
    "        tokenizer_result = self._tokenizer.tokenize(\n",
    "            tokenizer_input, max_len=self._max_len)\n",
    "\n",
    "        return DatasetItem(\n",
    "            id=[id],\n",
    "            x=tokenizer_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT = os.getenv('__KGLTBX_ENVIRONMENT', 'laptop')\n",
    "_env = _env = Environment(ENVIRONMENT)\n",
    "\n",
    "TARGET_LIST = [\n",
    "    'cohesion',\n",
    "    'syntax',\n",
    "    'vocabulary',\n",
    "    'phraseology',\n",
    "    'grammar',\n",
    "    'conventions',\n",
    "]\n",
    "\n",
    "SEED = 42\n",
    "NUM_FOLDS = 5\n",
    "DEVICE = CUDADevice()\n",
    "MAX_LEN = 1024\n",
    "BATCH_SIZE = _env.param(\n",
    "    kaggle=8,\n",
    "    # colab=4,\n",
    "    laptop=2)\n",
    "NUM_WORKERS = _env.param(kaggle=2, colab=2, laptop=4)\n",
    "\n",
    "ROOT_DIR = _env.param(\n",
    "    kaggle=Path('/kaggle'),\n",
    "    laptop=Path('/kaggle'))\n",
    "DATA_DIR = _env.param(\n",
    "    kaggle=ROOT_DIR / 'input',\n",
    "    laptop=ROOT_DIR / 'data')\n",
    "MODEL_DIR = _env.param(\n",
    "    kaggle=DATA_DIR,\n",
    "    laptop=ROOT_DIR / 'models')\n",
    "FP_ELL_DATASET_DIR = _env.param(\n",
    "    kaggle=DATA_DIR / 'feedback-prize-english-language-learning',\n",
    "    laptop=DATA_DIR / 'fp-ell')\n",
    "INPUT_CSV_PATH = _env.param(\n",
    "    kaggle=FP_ELL_DATASET_DIR / 'test.csv',\n",
    "    laptop=FP_ELL_DATASET_DIR / 'train.csv')\n",
    "OUTPUT_CSV_PATH = _env.param(\n",
    "    kaggle=ROOT_DIR / 'working/submission.csv',\n",
    "    laptop=ROOT_DIR / 'submission/submission.csv')\n",
    "\n",
    "BACKBONE = 'microsoft/deberta-v3-base'\n",
    "BACKBONE_PATH = _env.param(\n",
    "    kaggle=str(DATA_DIR / 'deberta-v3-base/deberta-v3-base'),\n",
    "    laptop='microsoft/deberta-v3-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GPU model: {DEVICE.get_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pinning the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_data() -> pd.DataFrame:\n",
    "    return pd.read_csv(INPUT_CSV_PATH).iloc[:60]\n",
    "\n",
    "all_df = _read_data()\n",
    "all_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model builders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cohesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_cohesion_fold_model(fold: int) -> TransformerModel[TokenizerResult]:\n",
    "    backbone = Backbone.from_huggingface_checkpoint(BACKBONE_PATH, zero_out_dropout=True)\n",
    "    model = StandardModel(\n",
    "        backbone=backbone,\n",
    "        squeezer=TakeNthSqueezer(),\n",
    "        pooler=AttentionHeadPooler(backbone.out_dim_size),\n",
    "        dnn=torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(backbone.out_dim_size),\n",
    "            torch.nn.Linear(backbone.out_dim_size, 1),\n",
    "            SqueezeDim(),\n",
    "        ))\n",
    "    model.load_state_dict(torch.load(\n",
    "        MODEL_DIR / f'fp-ell-transformer-training-cohesion/cohesion-v1-layer_norm-ep_4-valfreq_0p25-pooler_att-full-fold_{fold}.pt'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_syntax_fold_model(fold: int) -> TransformerModel[TokenizerResult]:\n",
    "    backbone = Backbone.from_huggingface_checkpoint(BACKBONE_PATH, zero_out_dropout=True)\n",
    "    model = StandardModel(\n",
    "        backbone=backbone,\n",
    "        squeezer=TakeNthSqueezer(),\n",
    "        pooler=MeanPooler(),\n",
    "        dnn=torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(backbone.out_dim_size),\n",
    "            torch.nn.Linear(backbone.out_dim_size, 1),\n",
    "            SqueezeDim(),\n",
    "        ))\n",
    "    model.load_state_dict(torch.load(\n",
    "        MODEL_DIR / f'fp-ell-transformer-training-syntax/syntax-v1-layer_norm-ep_3-valfreq_0p25-full-fold_{fold}.pt'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_vocabulary_fold_model(fold: int) -> TransformerModel[TokenizerResult]:\n",
    "    backbone = Backbone.from_huggingface_checkpoint(BACKBONE_PATH, zero_out_dropout=True)\n",
    "    model = StandardModel(\n",
    "        backbone=backbone,\n",
    "        squeezer=TakeNthSqueezer(),\n",
    "        pooler=MeanPooler(),\n",
    "        dnn=torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(backbone.out_dim_size),\n",
    "            torch.nn.Linear(backbone.out_dim_size, 1),\n",
    "            SqueezeDim(),\n",
    "        ))\n",
    "    model.load_state_dict(torch.load(\n",
    "        MODEL_DIR / f'fp-ell-transformer-training-vocabulary/vocabulary-v1-layer_norm-ep_3-valfreq_0p25-std_init-fold_{fold}.pt'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phraseology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_phraseology_fold_model(fold: int) -> TransformerModel[TokenizerResult]:\n",
    "    backbone = Backbone.from_huggingface_checkpoint(BACKBONE_PATH, zero_out_dropout=True)\n",
    "    model = StandardModel(\n",
    "        backbone=backbone,\n",
    "        squeezer=TakeNthSqueezer(),\n",
    "        pooler=MeanPooler(),\n",
    "        dnn=torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(backbone.out_dim_size),\n",
    "            torch.nn.Linear(backbone.out_dim_size, 1),\n",
    "            SqueezeDim(),\n",
    "        ))\n",
    "    model.load_state_dict(torch.load(\n",
    "        MODEL_DIR / f'fp-ell-transformer-training-phraseology/phraseology-v1-layer_norm-ep_3-valfreq_0p25-std_init-full-fold_{fold}.pt'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_grammar_fold_model(fold: int) -> TransformerModel[TokenizerResult]:\n",
    "    backbone = Backbone.from_huggingface_checkpoint(BACKBONE_PATH, zero_out_dropout=True)\n",
    "    model = StandardModel(\n",
    "        backbone=backbone,\n",
    "        squeezer=ConcatSqueezer([9, 10, 11, 12]),\n",
    "        pooler=MeanPooler(),\n",
    "        dnn=torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(backbone.out_dim_size * 4),\n",
    "            torch.nn.Linear(backbone.out_dim_size * 4, 1),\n",
    "            SqueezeDim(),\n",
    "        ))\n",
    "    model.load_state_dict(torch.load(\n",
    "        MODEL_DIR / f'fp-ell-transformer-training-grammar/grammar-v1-lnorm-ep_4-valfreq_0p25-sqzr_cat_9_to_12-full-fold_{fold}.pt'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_conventions_fold_model(fold: int) -> TransformerModel[TokenizerResult]:\n",
    "    backbone = Backbone.from_huggingface_checkpoint(BACKBONE_PATH, zero_out_dropout=True)\n",
    "    model = StandardModel(\n",
    "        backbone=backbone,\n",
    "        squeezer=TakeNthSqueezer(),\n",
    "        pooler=MeanPooler(),\n",
    "        dnn=torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(backbone.out_dim_size),\n",
    "            torch.nn.Linear(backbone.out_dim_size, 1),\n",
    "            SqueezeDim(),\n",
    "        ))\n",
    "    model.load_state_dict(torch.load(\n",
    "        MODEL_DIR / f'fp-ell-transformer-training-conventions/conventions-v1-layer_norm-ep_3-valfreq_0p25-full-fold_{fold}.pt'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_by_model(df: pd.DataFrame, model_builder: t.Callable[[], TransformerModel[TokenizerResult]]) -> PredDict:\n",
    "    tokenizer = get_tokenizer_for_backbone(\n",
    "        backbone=BACKBONE,\n",
    "        checkpoint=BACKBONE_PATH,\n",
    "        padding_strategy=PaddingStrategy.DO_NOT_PAD)\n",
    "    predictor = StandardPredictor(\n",
    "        model=model_builder(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        collator=DatasetItemCollator(\n",
    "            id_collate_fn=lambda x: sum(x, []),\n",
    "            x_collate_fn=TokenizerResultCollator(DataCollatorWithPadding(tokenizer.tokenizer))),\n",
    "        device=DEVICE,\n",
    "        progress_bar=NotebookProgressBar())\n",
    "\n",
    "    dataset = Dataset(df, tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "    return predictor.predict(dataset)\n",
    "\n",
    "\n",
    "def _predict(\n",
    "        df: pd.DataFrame,\n",
    "        model_builder_list: t.List[t.Callable[[], TransformerModel[TokenizerResult]]],\n",
    "        ensembling_strategy: EnsemblingStrategy) -> PredDict:\n",
    "    return ensembling_strategy.ensemble([\n",
    "        _predict_by_model(df=df, model_builder=model_builder)\n",
    "        for model_builder in model_builder_list\n",
    "    ])\n",
    "\n",
    "\n",
    "cohesion_df = pd.DataFrame([\n",
    "    {'text_id': id, 'cohesion_lvl1_score': score}\n",
    "    for id, (score,) in _predict(\n",
    "        df=all_df,\n",
    "        model_builder_list=[\n",
    "            functools.partial(_build_cohesion_fold_model, fold=fold)\n",
    "            for fold in range(NUM_FOLDS)\n",
    "        ],\n",
    "        ensembling_strategy=MeanEnsemblingStrategy()).items()\n",
    "])\n",
    "syntax_df = pd.DataFrame([\n",
    "    {'text_id': id, 'syntax_lvl1_score': score}\n",
    "    for id, (score,) in _predict(\n",
    "        df=all_df,\n",
    "        model_builder_list=[\n",
    "            functools.partial(_build_syntax_fold_model, fold=fold)\n",
    "            for fold in range(NUM_FOLDS)\n",
    "        ],\n",
    "        ensembling_strategy=MeanEnsemblingStrategy()).items()\n",
    "])\n",
    "vocabulary_df = pd.DataFrame([\n",
    "    {'text_id': id, 'vocabulary_lvl1_score': score}\n",
    "    for id, (score,) in _predict(\n",
    "        df=all_df,\n",
    "        model_builder_list=[\n",
    "            functools.partial(_build_vocabulary_fold_model, fold=fold)\n",
    "            for fold in range(NUM_FOLDS)\n",
    "        ],\n",
    "        ensembling_strategy=MeanEnsemblingStrategy()).items()\n",
    "])\n",
    "phraseology_df = pd.DataFrame([\n",
    "    {'text_id': id, 'phraseology_lvl1_score': score}\n",
    "    for id, (score,) in _predict(\n",
    "        df=all_df,\n",
    "        model_builder_list=[\n",
    "            functools.partial(_build_phraseology_fold_model, fold=fold)\n",
    "            for fold in range(NUM_FOLDS)\n",
    "        ],\n",
    "        ensembling_strategy=MeanEnsemblingStrategy()).items()\n",
    "])\n",
    "grammar_df = pd.DataFrame([\n",
    "    {'text_id': id, 'grammar_lvl1_score': score}\n",
    "    for id, (score,) in _predict(\n",
    "        df=all_df,\n",
    "        model_builder_list=[\n",
    "            functools.partial(_build_grammar_fold_model, fold=fold)\n",
    "            for fold in range(NUM_FOLDS)\n",
    "        ],\n",
    "        ensembling_strategy=MeanEnsemblingStrategy()).items()\n",
    "])\n",
    "conventions_df = pd.DataFrame([\n",
    "    {'text_id': id, 'conventions_lvl1_score': score}\n",
    "    for id, (score,) in _predict(\n",
    "        df=all_df,\n",
    "        model_builder_list=[\n",
    "            functools.partial(_build_conventions_fold_model, fold=fold)\n",
    "            for fold in range(NUM_FOLDS)\n",
    "        ],\n",
    "        ensembling_strategy=MeanEnsemblingStrategy()).items()\n",
    "])\n",
    "all_df = all_df\\\n",
    "    .merge(cohesion_df, left_on='text_id', right_on='text_id')\\\n",
    "    .merge(syntax_df, left_on='text_id', right_on='text_id')\\\n",
    "    .merge(vocabulary_df, left_on='text_id', right_on='text_id')\\\n",
    "    .merge(phraseology_df, left_on='text_id', right_on='text_id')\\\n",
    "    .merge(grammar_df, left_on='text_id', right_on='text_id')\\\n",
    "    .merge(conventions_df, left_on='text_id', right_on='text_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LVL1_SCORE_FEATURE_LIST = [f'{target}_lvl1_score' for target in TARGET_LIST]\n",
    "_FEATURE_GENERATOR_LIST = [\n",
    "    # Score-based\n",
    "    *features.L1Distance.pairwise_from_feature_list(_LVL1_SCORE_FEATURE_LIST),\n",
    "    features.Mean(name='lvl1_mean', feature_list=_LVL1_SCORE_FEATURE_LIST),\n",
    "    features.Stdev(name='lvl1_std', feature_list=_LVL1_SCORE_FEATURE_LIST),\n",
    "    # Custom simple\n",
    "    text_features.SubstrCount(name='num_commas', substr=','),\n",
    "    text_features.SubstrCount(name='num_dots', substr='.'),\n",
    "    text_features.SubstrCount(name='num_colons', substr=':'),\n",
    "    text_features.SubstrCount(name='num_semicolons', substr=';'),\n",
    "    text_features.SubstrCount(name='num_ellipsis', substr='...'),\n",
    "    text_features.SubstrCount(name='num_newlines', substr='\\n'),\n",
    "    text_features.SubstrCount(name='num_spaces', substr=' '),\n",
    "    # TextStat simple\n",
    "    text_features.Func(name='syllable_count', func=textstat.syllable_count),\n",
    "    text_features.Func(name='lexicon_count', func=functools.partial(textstat.lexicon_count, removepunct=True)),\n",
    "    text_features.Func(name='char_count', func=functools.partial(textstat.char_count, ignore_spaces=True)),\n",
    "    text_features.Func(name='letter_count', func=functools.partial(textstat.letter_count, ignore_spaces=True)),\n",
    "    text_features.Func(name='polysyllabcount', func=functools.partial(textstat.polysyllabcount)),\n",
    "    text_features.Func(name='monosyllabcount', func=functools.partial(textstat.monosyllabcount)),\n",
    "    # Custom complex\n",
    "    features.Div(name='ratio_commas', lhs_feature='num_commas', rhs_feature='char_count'),\n",
    "    features.Div(name='ratio_dots', lhs_feature='num_dots', rhs_feature='char_count'),\n",
    "    features.Div(name='ratio_colons', lhs_feature='num_colons', rhs_feature='char_count'),\n",
    "    features.Div(name='ratio_semicolons', lhs_feature='num_semicolons', rhs_feature='char_count'),\n",
    "    features.Div(name='ratio_ellipsis', lhs_feature='num_ellipsis', rhs_feature='char_count'),\n",
    "    features.Div(name='ratio_newlines', lhs_feature='num_newlines', rhs_feature='char_count'),\n",
    "    features.Div(name='ratio_spaces', lhs_feature='num_spaces', rhs_feature='char_count'),\n",
    "    # TextStat complex\n",
    "    text_features.Func(name='flesch_reading_ease', func=textstat.flesch_reading_ease),\n",
    "    text_features.Func(name='flesch_kincaid_grade', func=textstat.flesch_kincaid_grade),\n",
    "    text_features.Func(name='gunning_fog', func=textstat.gunning_fog),\n",
    "    text_features.Func(name='smog_index', func=textstat.smog_index),\n",
    "    text_features.Func(name='automated_readability_index', func=textstat.automated_readability_index),\n",
    "    text_features.Func(name='coleman_liau_index', func=textstat.coleman_liau_index),\n",
    "    text_features.Func(name='linsear_write_formula', func=textstat.linsear_write_formula),\n",
    "    text_features.Func(name='dale_chall_readability_score', func=textstat.dale_chall_readability_score),\n",
    "    text_features.Func(name='text_standard', func=functools.partial(textstat.text_standard, float_output=True)),  # type: ignore\n",
    "    text_features.Func(name='spache_readability', func=textstat.spache_readability),\n",
    "    text_features.Func(name='mcalpine_eflaw', func=textstat.mcalpine_eflaw),\n",
    "    text_features.Func(name='reading_time', func=functools.partial(textstat.reading_time, ms_per_char=14.69)),\n",
    "]\n",
    "\n",
    "def build_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    text_srs = df['full_text']\n",
    "\n",
    "    feature_arr_dict = text_features.generate_text_features(\n",
    "        generator_list=_FEATURE_GENERATOR_LIST,\n",
    "        text_seq=text_srs.tolist(),\n",
    "        init_feature_array_dict={\n",
    "            f'{target}_lvl1_score': df[f'{target}_lvl1_score'].values\n",
    "            for target in TARGET_LIST\n",
    "        })  # type: ignore\n",
    "    for feature_name, feature_arr in feature_arr_dict.items():\n",
    "        df[feature_name] = feature_arr\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "all_df = build_features(all_df)\n",
    "lvl1_features = all_df[[col for col in all_df.columns if col not in {'text_id', 'full_text'}]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lvl2(feature_arr: np.ndarray, target: str) -> np.ndarray:\n",
    "    pred_arr_list = []\n",
    "    for fold in range(NUM_FOLDS):\n",
    "        regressor = CatBoostRegressor(task_type='GPU').load_model(\n",
    "            str(MODEL_DIR / f'fp-ell-models-boosting/lvl2-catboost-{target}-cv1-fold_{fold}.cbm'))\n",
    "        pred_arr_list.append(regressor.predict(feature_arr))\n",
    "    return np.stack(pred_arr_list, axis=0).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['cohesion'] = predict_lvl2(lvl1_features, target='cohesion')\n",
    "all_df['syntax'] = predict_lvl2(lvl1_features, target='syntax')\n",
    "all_df['vocabulary'] = predict_lvl2(lvl1_features, target='vocabulary')\n",
    "all_df['phraseology'] = predict_lvl2(lvl1_features, target='phraseology')\n",
    "all_df['grammar'] = predict_lvl2(lvl1_features, target='grammar')\n",
    "all_df['conventions'] = all_df['conventions_lvl1_score']\n",
    "\n",
    "pred_df = all_df[['text_id', *TARGET_LIST]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(OUTPUT_CSV_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('kaggle-fp-ell')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4646e3fde12bd3179c3551877f577659b7a8fa6d1b23b85720655ff3fa8cde14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
