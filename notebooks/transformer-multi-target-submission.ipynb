{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if 'KAGGLE_URL_BASE' in os.environ:\n",
    "    print('Running on Kaggle so initializing the environment...')\n",
    "\n",
    "    os.environ['__KGLTBX_ENVIRONMENT'] = 'kaggle'\n",
    "    sys.path.append('/kaggle/input/kaggle-toolbox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from kaggle_toolbox.data import DatasetItem, Movable\n",
    "from kaggle_toolbox.device import CUDADevice\n",
    "from kaggle_toolbox.ensembling import EnsemblingStrategy, MeanEnsemblingStrategy\n",
    "from kaggle_toolbox.environment import Environment\n",
    "from kaggle_toolbox.nlp.transformer import Backbone, Model as TransformerModel, StandardModel, \\\n",
    "    AttentionHeadPooler, TakeNthSqueezer, get_tokenizer_for_backbone, Tokenizer, TokenizerResult, \\\n",
    "    TokenizerResultCollator, seed_everything\n",
    "from kaggle_toolbox.prediction import PredDict\n",
    "from kaggle_toolbox.predictor import StandardPredictor\n",
    "from kaggle_toolbox.progress import NotebookProgressBar\n",
    "from kaggle_toolbox.typing import DynamicDict\n",
    "from torch.utils.data import Dataset as TorchDataset, default_collate as default_collate_fn\n",
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "from transformers.utils.generic import PaddingStrategy\n",
    "from transformers.utils.logging import set_verbosity_error as set_transformers_verbosity_error\n",
    "\n",
    "\n",
    "NotebookProgressBar.attach_to_pandas()\n",
    "set_transformers_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = t.TypeVar('_X', bound=Movable)\n",
    "\n",
    "class DatasetItemCollator(t.Generic[_X]):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            x_collate_fn: t.Callable[[t.List[_X]], _X],\n",
    "            id_collate_fn: t.Callable[[t.List[t.List[str]]], t.List[str]] = default_collate_fn):\n",
    "        self._x_collate_fn = x_collate_fn\n",
    "        self._id_collate_fn = id_collate_fn\n",
    "\n",
    "    def __call__(self, item_list: t.List[DatasetItem[_X]]) -> DatasetItem[_X]:\n",
    "        return DatasetItem(\n",
    "            id=self._id_collate_fn([item.id for item in item_list]),\n",
    "            x=self._x_collate_fn([item.x for item in item_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(TorchDataset[DatasetItem[TokenizerResult]]):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            tokenizer: Tokenizer,\n",
    "            max_len: int):\n",
    "        self._df = df.copy().reset_index(drop=True)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "\n",
    "    def _get_tokenizer_input(self, row: DynamicDict) -> str:\n",
    "        (\n",
    "            full_text,\n",
    "         ) = (\n",
    "            row.get_typed_or_raise('full_text', str),\n",
    "         )\n",
    "\n",
    "        return full_text\n",
    "\n",
    "    def sort_by_tokenizer_input_len(self):\n",
    "        self._df['_tok_input_len'] = self._df.progress_apply(\n",
    "            lambda row: self._get_tokenizer_input(DynamicDict(t.cast(t.Dict[str, t.Any], row))), axis=1)\n",
    "        self._df = self._df.sort_values('_tok_input_len')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> DatasetItem[TokenizerResult]:\n",
    "        row = self._df.iloc[idx]\n",
    "\n",
    "        tokenizer_input = self._get_tokenizer_input(DynamicDict(t.cast(t.Dict[str, t.Any], row)))\n",
    "        id = str(row['text_id'])\n",
    "\n",
    "        tokenizer_result = self._tokenizer.tokenize(\n",
    "            tokenizer_input, max_len=self._max_len)\n",
    "\n",
    "        return DatasetItem(\n",
    "            id=[id],\n",
    "            x=tokenizer_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT = os.getenv('__KGLTBX_ENVIRONMENT', 'laptop')\n",
    "_env = _env = Environment(ENVIRONMENT)\n",
    "\n",
    "TARGET_LIST = [\n",
    "    'cohesion',\n",
    "    'syntax',\n",
    "    'vocabulary',\n",
    "    'phraseology',\n",
    "    'grammar',\n",
    "    'conventions',\n",
    "]\n",
    "\n",
    "SEED = 42\n",
    "NUM_FOLDS = 5\n",
    "DEVICE = CUDADevice()\n",
    "SINGLE_TARGET_MAX_LEN = 1024\n",
    "OWN_MULTI_TARGET_MAX_LEN = 1428\n",
    "KOJ_MULTI_TARGET_MAX_LEN = 1428\n",
    "BATCH_SIZE = _env.param(\n",
    "    kaggle=8,\n",
    "    # colab=4,\n",
    "    laptop=2)\n",
    "NUM_WORKERS = _env.param(kaggle=2, colab=2, laptop=4)\n",
    "\n",
    "ROOT_DIR = _env.param(\n",
    "    kaggle=Path('/kaggle'),\n",
    "    laptop=Path('/kaggle'))\n",
    "DATA_DIR = _env.param(\n",
    "    kaggle=ROOT_DIR / 'input',\n",
    "    laptop=ROOT_DIR / 'data')\n",
    "MODEL_DIR = _env.param(\n",
    "    kaggle=DATA_DIR,\n",
    "    laptop=ROOT_DIR / 'models')\n",
    "FP_ELL_DATASET_DIR = _env.param(\n",
    "    kaggle=DATA_DIR / 'feedback-prize-english-language-learning',\n",
    "    laptop=DATA_DIR / 'fp-ell')\n",
    "INPUT_CSV_PATH = _env.param(\n",
    "    kaggle=FP_ELL_DATASET_DIR / 'test.csv',\n",
    "    laptop=FP_ELL_DATASET_DIR / 'train.csv')\n",
    "OUTPUT_CSV_PATH = _env.param(\n",
    "    kaggle=ROOT_DIR / 'working/submission.csv',\n",
    "    laptop=ROOT_DIR / 'submission/submission.csv')\n",
    "\n",
    "BACKBONE = 'microsoft/deberta-v3-base'\n",
    "BACKBONE_PATH = _env.param(\n",
    "    kaggle=str(DATA_DIR / 'deberta-v3-base/deberta-v3-base'),\n",
    "    laptop='microsoft/deberta-v3-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GPU model: {DEVICE.get_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pinning the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_data() -> pd.DataFrame:\n",
    "    return pd.read_csv(INPUT_CSV_PATH)\n",
    "\n",
    "all_df = _read_data()\n",
    "all_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KOJ Multi Target Model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim: int):\n",
    "        super().__init__()\n",
    "        self.attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, in_dim),\n",
    "            torch.nn.LayerNorm(in_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(in_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, last_hidden_state: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        w = self.attention(last_hidden_state).float()\n",
    "        w[attention_mask==0]=float('-inf')\n",
    "        w = torch.softmax(w,1)\n",
    "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
    "        return attention_embeddings\n",
    "\n",
    "\n",
    "class KOJMultiTargetModel(TransformerModel[TokenizerResult]):\n",
    "    def __init__(self, backbone_checkpoint: str):\n",
    "        super().__init__()\n",
    "        config = AutoConfig.from_pretrained(backbone_checkpoint)\n",
    "        self.model = AutoModel.from_pretrained(backbone_checkpoint, config=config)\n",
    "        self.pool = AttentionPooling(config.hidden_size)\n",
    "        self.fc = torch.nn.Linear(config.hidden_size, len(TARGET_LIST))\n",
    "\n",
    "    def feature(self, x: TokenizerResult) -> torch.Tensor:\n",
    "        outputs = self.model(**x.tensor_dict)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, x.attention_mask)\n",
    "        return feature\n",
    "    \n",
    "    def forward(self, x: TokenizerResult) -> torch.Tensor:\n",
    "        feature = self.feature(x)\n",
    "        outout = self.fc(feature)\n",
    "        return outout\n",
    "\n",
    "\n",
    "def _build_koj_multi_target_fold_model(fold: int) -> TransformerModel[TokenizerResult]:\n",
    "    model = KOJMultiTargetModel(backbone_checkpoint=BACKBONE_PATH)\n",
    "    model.load_state_dict(torch.load(\n",
    "        MODEL_DIR / f'fp-ell-koj-awp-v3-base/microsoft-deberta-v3-base_fold{fold}_best.pth')['model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Own Multi Target Model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_own_multi_target_fold_model(fold: int) -> TransformerModel[TokenizerResult]:\n",
    "    backbone = Backbone.from_huggingface_checkpoint(BACKBONE_PATH, zero_out_dropout=True)\n",
    "    model = StandardModel(\n",
    "        backbone=backbone,\n",
    "        squeezer=TakeNthSqueezer(),\n",
    "        pooler=AttentionHeadPooler(backbone.out_dim_size),\n",
    "        dnn=torch.nn.Sequential(\n",
    "            torch.nn.Linear(backbone.out_dim_size, len(TARGET_LIST))\n",
    "        ))\n",
    "    model.load_state_dict(torch.load(\n",
    "        MODEL_DIR / f'fp-ell-transformer-training-multi-target/multi-koj-fold_{fold}.pt'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_by_model(\n",
    "        df: pd.DataFrame,\n",
    "        model_builder: t.Callable[[], TransformerModel[TokenizerResult]],\n",
    "        max_len: int) -> PredDict:\n",
    "    tokenizer = get_tokenizer_for_backbone(\n",
    "        backbone=BACKBONE,\n",
    "        checkpoint=BACKBONE_PATH,\n",
    "        padding_strategy=PaddingStrategy.DO_NOT_PAD)\n",
    "    predictor = StandardPredictor(\n",
    "        model=model_builder(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        collator=DatasetItemCollator(\n",
    "            id_collate_fn=lambda x: sum(x, []),\n",
    "            x_collate_fn=TokenizerResultCollator(DataCollatorWithPadding(tokenizer.tokenizer))),\n",
    "        device=DEVICE,\n",
    "        progress_bar=NotebookProgressBar())\n",
    "\n",
    "    dataset = Dataset(df, tokenizer=tokenizer, max_len=max_len)\n",
    "    return predictor.predict(dataset)\n",
    "\n",
    "\n",
    "def _predict(\n",
    "        df: pd.DataFrame,\n",
    "        model_builder_list: t.List[t.Callable[[], TransformerModel[TokenizerResult]]],\n",
    "        ensembling_strategy: EnsemblingStrategy,\n",
    "        max_len: int) -> PredDict:\n",
    "    return ensembling_strategy.ensemble([\n",
    "        _predict_by_model(df=df, model_builder=model_builder, max_len=max_len)\n",
    "        for model_builder in model_builder_list\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_df = pd.DataFrame([\n",
    "#     {'text_id': id, **{k: v for k, v in zip(TARGET_LIST, score_list)}}\n",
    "#     for id, score_list in _predict(\n",
    "#         df=all_df,\n",
    "#         model_builder_list=[\n",
    "#             functools.partial(_build_own_multi_target_fold_model, fold=fold)\n",
    "#             for fold in range(NUM_FOLDS)\n",
    "#         ],\n",
    "#         ensembling_strategy=MeanEnsemblingStrategy(),\n",
    "#         max_len=OWN_MULTI_TARGET_MAX_LEN,\n",
    "#     ).items()\n",
    "# ])\n",
    "\n",
    "# pred_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame([\n",
    "    {'text_id': id, **{k: v for k, v in zip(TARGET_LIST, score_list)}}\n",
    "    for id, score_list in _predict(\n",
    "        df=all_df,\n",
    "        model_builder_list=[\n",
    "            functools.partial(_build_koj_multi_target_fold_model, fold=fold)\n",
    "            for fold in range(4)\n",
    "        ],\n",
    "        ensembling_strategy=MeanEnsemblingStrategy(),\n",
    "        max_len=KOJ_MULTI_TARGET_MAX_LEN,\n",
    "    ).items()\n",
    "])\n",
    "\n",
    "pred_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df[['text_id', *TARGET_LIST]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(OUTPUT_CSV_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('kaggle-fp-ell')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4646e3fde12bd3179c3551877f577659b7a8fa6d1b23b85720655ff3fa8cde14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
